{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bb3b2c-faeb-41ec-8499-508a8bc71b76",
   "metadata": {},
   "source": [
    "# L2: Role of the Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071d512-d667-49ef-9e74-43cd88764e36",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#edfbff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> Tokenizers are trainable components and they <b><font color=\"green\">learn</font></b> the best <b><font color=\"green\"><b>vocabulary</font></b> for the given training data. Several tokenizing algorithms exist. They are fully <b>deterministic</b>, contrary to the neural networks. They depend on the <b>statistics</b> of the input data. By <b>\"Vocabulary\"</b>, we mean the final set of <b>distinct</b> output tokens that we get after tokenizing.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c6cb6-2ec6-4193-b404-39ea88b95011",
   "metadata": {},
   "source": [
    "### Variation of tokenizers and hyperparameters used in different embedding models\n",
    "The table below shows different embedding models and the tokenizing algorithm they use. <b>vocabulary size</b> is a hyperparameter\n",
    "for tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcabec9-132e-4ee7-a08c-86b4b8b25552",
   "metadata": {},
   "source": [
    "![title](img/l2_emb_models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b197c-ec6a-4b74-8cb8-46428c704e55",
   "metadata": {},
   "source": [
    "## BPE - Byte-Pair Encoding\n",
    "- Divide the sentence(s) into unique characters or bytes\n",
    "- The list of tokens = our <b><font color=\"green\"><b>vocabulary</font></b>\n",
    "- Add the most occuring pair of tokens from previous step to the existing list of tokens\n",
    "- Treat the newly coupled token as a `new` and `separate` token\n",
    "- Repeat adding newly paired tokens\n",
    "- Repeat adding new tokens until we reach our <b>vocabulary size</b>\n",
    "\n",
    "Hands on demonstration :\n",
    "https://youtu.be/HEikzVL-lZU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe831d-9870-42a3-bc7a-391dd039aba3",
   "metadata": {},
   "source": [
    "### Tokenize a new text\n",
    "\n",
    "After creating our vocabulary from the given corpus using the <b>BPE Tokenizer</b>, we can now tokenize new text\n",
    "\n",
    "1. Use the <b><font color=\"green\"><b>longest subword</b> token from the vocab\n",
    "2. Match the token with the words in the given sentence starting from left\n",
    "3. When the subword matches, divide the word into the format of <b>matched subword + remainder</b>\n",
    "4. Match and divide one full iteration with this subword token until the end of the sentence\n",
    "5. Repeat the process with the <b>next largest subword token</b>\n",
    "6. Repeat until the vocabulary is exhausted\n",
    "  \n",
    "#### <font color=\"green\"><b>Example : </b></font>\n",
    "- Example sentence : \"she walked\"\n",
    "- Some top tokens out of the 14 sized vocabulary \"walke\" \"walk\" \"wal\" \"alk\" <b>...</b> \"e\" \"k\" (based on the most frequent occurrences)\n",
    "- We take \"<font color=\"red\"><b>walk</b></font>\", compare from the left of the sentence, and find she <font color=\"red\"><b>walk</b></font>ed\n",
    "- Tokenize the word as <font color=\"blue\"><b>|walke|</b></font> + remainder <font color=\"blue\"><b>\"d\"</b></font>\n",
    "- We take the next top token \"walk\" and find other untokenized words in the sentence\n",
    "- We repeat the searching and tokenizing steps\n",
    "- The remainder of the word \"walked\", which is <font color=\"blue\"><b>\"d\"</b></font> is matched later with token \"d\", so it is also included in the list of tokenized words\n",
    "- When the token is \"<font color=\"red\"><b>e</b></font>\" and the first untokenized word comes as \"<font color=\"red\"><b>she</b></font>\", the <b>s & h</b> is considered as <b>unknown</b> tokens and are ignored afterwards, so we remain with  |<font color=\"blue\"><b>e</b></font>|\n",
    "- After all the steps, we get the tokenized sentence as <font color=\"blue\"><b>|e| |walke| |d|</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb3dcf2-4003-414c-a3c4-1e155098be80",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e79d2302-3983-4e72-a6b0-262d63fd57ad",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "training_data = [\"walker walked a long walk\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5379f6-be97-4dcf-98a3-060d88d4d3c6",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "bpe_tokenizer = Tokenizer(BPE())\n",
    "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "bpe_trainer = BpeTrainer(vocab_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4289b9-1761-4dce-8faf-b4be05dab0b2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "bpe_tokenizer.train_from_iterator(training_data, bpe_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57787cee-ffb1-4a0d-9d53-81903d6acaf3",
   "metadata": {},
   "source": [
    "By running <font color=\"blue\">bpe_tokenizer.get_vocab()</font> after tokenizer training, we can see the learned tokens from the training data. As the training process is **iterative**, we can get the order of the tokens learned one after another by looking at their corresponding **ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd44f86-2aab-46f1-99ec-ea73eac92c7d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'k': 4,\n",
       " 'l': 5,\n",
       " 'o': 7,\n",
       " 'al': 10,\n",
       " 'd': 1,\n",
       " 'n': 6,\n",
       " 'wal': 11,\n",
       " 'w': 9,\n",
       " 'walk': 12,\n",
       " 'walke': 13,\n",
       " 'r': 8,\n",
       " 'e': 2,\n",
       " 'g': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check what are the learned tokens or vocabulary after the tokenizer training\n",
    "bpe_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a3b18-272e-4ac8-88cd-09407d8d31d2",
   "metadata": {},
   "source": [
    "#### Encode (<font color=\"red\">= Tokenize</font>) new texts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f022ee-a6f0-444f-a7f9-d8448e01db91",
   "metadata": {},
   "source": [
    "This encoder only tries to exact match the tokens out of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74912810-c4ef-4373-86d6-77a095ef01ec",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walke', 'r', 'walke', 'd', 'a', 'l', 'o', 'n', 'g', 'walk']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "671646d9-3403-4ef7-ae20-72af7fbdad96",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 5, 4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"wlk\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22aced61-6e22-4ebe-bd35-60509e744aa2",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'l', 'k']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e02ebb5-e9e2-4613-948a-655775249ea6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'walke', 'd']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57b19e32-0537-4b5c-80f3-539d083a6376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'al', 'k']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"she talks\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a65b0-5bf1-4b6a-92cd-651e678a8087",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#edfbff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> The <b>Tokenizers</b> library introduces us with different types of tokenizers - <b><font color=\"green\"><b>BPE, WordPiece and Unigram</font></b> <br>\n",
    "Tokenizer library used : <br>\n",
    "https://github.com/huggingface/tokenizers <br>\n",
    "The tokenizers work in the following way <br> </p>\n",
    "<ul style=\"background-color:#edfbff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "    <li style=\"margin-left:10px\">Instantiate the required tokenizer, for example - BPE()</li>\n",
    "    <li style=\"margin-left:10px\">The tokenizer does not have any vocabulary to start with</li>\n",
    "    <li style=\"margin-left:10px\">The Whitespace() pretokenizer splits the text into words based on whitespace seperators</li>\n",
    "    <li style=\"margin-left:10px\">Use a trainer i.e - BpeTrainer</li>\n",
    "    <li style=\"margin-left:10px\">The trainer learns the vocabulary, trains the tokenizer model</li>\n",
    "    <li style=\"margin-left:10px\">Pass the trainer and training_data to the tokenizer and train using a python iterator (train_from_iterator method)</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1fc751-1074-4c56-a68f-ae6e6491c227",
   "metadata": {},
   "source": [
    "## WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db1f12-dd5a-4971-84c4-0bb96088a00e",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#edfbff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "The only difference between <b>WordPiece</b> and <b>BPE</b> is that wordpiece differentiates between initial and middle letters / subword inside a word by adding <b>## prefix</b> to the middle tokens. The general idea of this algorithm is to learn words, prefixes and suffixes separately. The basic steps are the same as BPE, but only t\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45201707-ade9-4996-953d-16a803c5be92",
   "metadata": {},
   "source": [
    "![title](img/l2_wordpiece_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be26a0c5-05bc-4c16-bace-904c0247161a",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#fad7ac; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "    <li style=\"margin-left:10px\">Break down the corpus into <b>distinct</b> individual letter tokens</li>\n",
    "    <li style=\"margin-left:10px\">Traverse from left to right of the corpus</li>\n",
    "    <li style=\"margin-left:10px\">At each iteration, analyze a <b><font color=\"green\">pair</font> of single tokens <b>(\"h\", \"##u\" from the word \"hugging\")</b> </li>\n",
    "    <li style=\"margin-left:10px\">Record the score of this pair in a table</li>\n",
    "    <li style=\"margin-left:10px\">Record all such score of sequential pairs in the traversed corpus</li>\n",
    "    <li style=\"margin-left:10px\">Select the highest scoring potential pair, suppose : <b>(\"l\", \"##e\" from the words \"learner\" or \"learn\")</b></li>\n",
    "    <li style=\"margin-left:10px\">Join the pair <b>(\"l\", \"##e\" -> \"le\")</b></li>\n",
    "    <li style=\"margin-left:10px\">Replace all cooccurrences of \"l\" and \"##e\" with \"le\" in the vocabulary</b></li>\n",
    "    <li style=\"margin-left:10px\">Repeat the steps of scoring and updating until reaching the vocabulary size</b></li>\n",
    "<ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c081bd-e69d-4a67-8631-0738a563d2d5",
   "metadata": {},
   "source": [
    "![title](img/l2_wordpiece_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a96f9-b671-4d56-8d0f-b34f6a43b230",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#faecac; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">Let’s say we have the word \"walker\" and the subwords \"wa\", \"lk\", \"er\" and we are at a point where we need to decide what to merge.\n",
    "Suppose [\"wa\"] appears frequently, but [\"wa\"] and [\"lk\"] together aren't as frequent.\n",
    "On the other hand, [\"lk\"] and [\"er\"] occur together often, e.g., in words like \"walker\", \"talker\", \"stalker\".\n",
    "Based on the scoring mechanism, the tokenizer will prioritize merging [\"lk\"] and [\"er\"] into [\"lker\"] (or eventually [\"walk\", \"##er\"]), because this pair maximizes the likelihood of the text, compared to merging [\"wa\"] and [\"lk\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bbdde63-90d6-493c-8316-96869c366a99",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "from real_wordpiece.trainer import RealWordPieceTrainer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "real_wordpiece_tokenizer = Tokenizer(WordPiece())\n",
    "real_wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# The training algorithm is different than the Huggingface's WordPiece\n",
    "real_wordpiece_trainer = RealWordPieceTrainer(vocab_size=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daef2453-f14d-4d77-9f33-b9285bd71920",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': 15,\n",
       " 'o': 16,\n",
       " 'g': 18,\n",
       " 'l': 6,\n",
       " '##g': 11,\n",
       " '##er': 22,\n",
       " '##r': 7,\n",
       " '##lk': 25,\n",
       " 'walk': 26,\n",
       " '##d': 8,\n",
       " '##l': 2,\n",
       " '##k': 3,\n",
       " 'n': 17,\n",
       " '##ng': 20,\n",
       " '##e': 4,\n",
       " 'lo': 19,\n",
       " 'long': 21,\n",
       " '##ed': 23,\n",
       " '##o': 9,\n",
       " 'k': 12,\n",
       " 'w': 0,\n",
       " '##n': 10,\n",
       " 'a': 5,\n",
       " 'wa': 24,\n",
       " '##a': 1,\n",
       " 'r': 14,\n",
       " 'e': 13}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_wordpiece_trainer.train_tokenizer(training_data, real_wordpiece_tokenizer)\n",
    "real_wordpiece_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29cc85bd-53c6-41b6-8fba-062ba9f82661",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', '##er', 'walk', '##ed', 'a', 'long', 'walk']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_wordpiece_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "455e1f41-e3a1-4366-bfac-44e6ac191aba",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', '##lk']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_wordpiece_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b04960-d9ba-4037-aeab-fe02f1af6d74",
   "metadata": {},
   "source": [
    "**Unknown Characters:**\n",
    "The following line will produce an error because it contains unknown characters. Please uncomment the line and run it to see the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bc73f36-ce2c-45d6-b2e6-834e7b7fe24d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "WordPiece error: Missing [UNK] token from the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mreal_wordpiece_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshe walked\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtokens\n",
      "\u001b[1;31mException\u001b[0m: WordPiece error: Missing [UNK] token from the vocabulary"
     ]
    }
   ],
   "source": [
    "real_wordpiece_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53930c4-e170-463d-b2e4-8def79ce164e",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#faecac; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">The prime difference between <b>RealWordPiece</b> and <b>Huggingface's WordPiece</b> is that the real one uses the <b>scoring function</b> while pairing to effectively detect prefix and suffixes. This enables the tokenization of new words into proper <b>base word and suffixes</b>. But the Huggingface one pairs the base tokens based on frequency of occurrences. <br>\n",
    "\"walker\" -> Huggingface -> \"walker\" (Largest frequently occurring token dominates)<br>\n",
    "\"walker\" -> RealWordPiece -> <b><font color=\"green\">\"walk\"</font></b> \"##er\" (Base word and suffix dominate) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d662e3-29fc-456c-9b75-1a35fdeaf330",
   "metadata": {},
   "source": [
    "## HuggingFace WordPiece and special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d0390-e863-46d5-aeeb-5a9ee2dc2a05",
   "metadata": {},
   "source": [
    "We can include an **UNK** token to handle unknown tokens in our vocab. After training, whenever any given text has any new text in it which is not part of the **trained vocabulary**, it matches with the **UNK** in the vocab then and tokenizes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6c9f6ec-7612-4fc0-9de5-fe9af772e8b4",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "unk_token = \"[UNK]\"\n",
    "\n",
    "wordpiece_model = WordPiece(unk_token=unk_token)\n",
    "wordpiece_tokenizer = Tokenizer(wordpiece_model)\n",
    "wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "wordpiece_trainer = WordPieceTrainer(vocab_size=28, special_tokens=[unk_token]) # 27 + 1 for the new UNK token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e99b0fe-8723-4193-aec4-7af50f4d296d",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 7,\n",
       " '##k': 13,\n",
       " '##lk': 21,\n",
       " 'walked': 27,\n",
       " 'a': 1,\n",
       " '##ng': 25,\n",
       " 'e': 3,\n",
       " '##d': 19,\n",
       " 'r': 9,\n",
       " 'w': 10,\n",
       " 'l': 6,\n",
       " '##o': 14,\n",
       " '##n': 15,\n",
       " 'wa': 20,\n",
       " '##e': 17,\n",
       " 'd': 2,\n",
       " '[UNK]': 0,\n",
       " '##l': 12,\n",
       " 'walk': 22,\n",
       " 'k': 5,\n",
       " 'g': 4,\n",
       " '##a': 11,\n",
       " 'o': 8,\n",
       " 'walke': 23,\n",
       " 'lo': 24,\n",
       " '##r': 18,\n",
       " 'walker': 26,\n",
       " '##g': 16}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.train_from_iterator(\n",
    "    training_data, \n",
    "    wordpiece_trainer\n",
    ")\n",
    "wordpiece_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "594722c8-24b0-496b-9859-943071bc7b78",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walker', 'walked', 'a', 'lo', '##ng', 'walk']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0abca87e-db68-498e-b137-d00a77f034b3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', '##lk']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccd22755-1115-4d9a-8ddb-3f3b2128941c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'walked']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1863790-b1f7-40a3-9fcd-cc4263530f7b",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963521e9-bab7-4021-b21a-4775f38c994b",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.models import Unigram\n",
    "\n",
    "unigram_tokenizer = Tokenizer(Unigram())\n",
    "unigram_tokenizer.pre_tokenizer = Whitespace()\n",
    "unigram_trainer = UnigramTrainer(\n",
    "    vocab_size=14, \n",
    "    special_tokens=[unk_token],\n",
    "    unk_token=unk_token,\n",
    ")\n",
    "\n",
    "unigram_tokenizer.train_from_iterator(training_data, unigram_trainer)\n",
    "unigram_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500af15-5de5-498b-b098-6d1554783257",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "unigram_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffe74e-968f-4b37-ab73-5686c18a8af7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "unigram_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee843700-b778-4af3-901c-53d420b12f51",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "unigram_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a62bb-7713-4e88-acb1-621e1da38203",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "unigram_tokenizer.encode(\"she walked\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3e6cc-4787-4922-a0f9-e98ac5eac5b9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db97e61-b26b-4628-bebd-94290afe3d45",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687670ec-9c81-4305-bb7e-377de2d50afa",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfba8a-6d99-4d12-b786-2a0a5fc9f0d4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a92bc-ff62-40de-8564-9ae78fee93e7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abef761-87c5-4402-9cd8-092f6a4e1e2a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8b6fc-ccac-4e52-96a7-a3fe610b62c8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
