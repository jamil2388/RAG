{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bb3b2c-faeb-41ec-8499-508a8bc71b76",
   "metadata": {},
   "source": [
    "# L2: Role of the Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071d512-d667-49ef-9e74-43cd88764e36",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#edfbff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> Tokenizers are trainable components and they <b><font color=\"green\">learn</font></b> the best <b><font color=\"green\"><b>vocabulary</font></b> for the given training data. Several tokenizing algorithms exist. They are fully <b>deterministic</b>, contrary to the neural networks. They depend on the <b>statistics</b> of the input data. By <b>\"Vocabulary\"</b>, we mean the final set of <b>distinct</b> output tokens that we get after tokenizing.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c6cb6-2ec6-4193-b404-39ea88b95011",
   "metadata": {},
   "source": [
    "### Variation of tokenizers and hyperparameters used in different embedding models\n",
    "The table below shows different embedding models and the tokenizing algorithm they use. <b>vocabulary size</b> is a hyperparameter\n",
    "for tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcabec9-132e-4ee7-a08c-86b4b8b25552",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/l2_emb_models.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b197c-ec6a-4b74-8cb8-46428c704e55",
   "metadata": {},
   "source": [
    "## BPE - Byte-Pair Encoding\n",
    "- Divide the sentence(s) into unique characters or bytes\n",
    "- The list of tokens = our <b><font color=\"green\"><b>vocabulary</font></b>\n",
    "- Add the most occuring pair of tokens from previous step to the existing list of tokens\n",
    "- Treat the newly coupled token as a `new` and `separate` token\n",
    "- Repeat adding newly paired tokens\n",
    "- Repeat adding new tokens until we reach our <b>vocabulary size</b>\n",
    "\n",
    "Hands on demonstration :\n",
    "https://youtu.be/HEikzVL-lZU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe831d-9870-42a3-bc7a-391dd039aba3",
   "metadata": {},
   "source": [
    "### Tokenize a new text\n",
    "\n",
    "After creating our vocabulary from the given corpus using the <b>BPE Tokenizer</b>, we can now tokenize new text\n",
    "\n",
    "1. Use the <b><font color=\"green\"><b>longest subword</b> token from the vocab\n",
    "2. Match the token with the words in the given sentence starting from left\n",
    "3. When the subword matches, divide the word into the format of <b>matched subword + remainder</b>\n",
    "4. Match and divide one full iteration with this subword token until the end of the sentence\n",
    "5. Repeat the process with the <b>next largest subword token</b>\n",
    "6. Repeat until the vocabulary is exhausted\n",
    "  \n",
    "#### <font color=\"green\"><b>Example : </b></font>\n",
    "- Example sentence : \"she walked\"\n",
    "- Some top tokens out of the 14 sized vocabulary \"walke\" \"walk\" \"wal\" \"alk\" <b>...</b> \"e\" \"k\" (based on the most frequent occurrences)\n",
    "- We take \"<font color=\"red\"><b>walk</b></font>\", compare from the left of the sentence, and find she <font color=\"red\"><b>walk</b></font>ed\n",
    "- Tokenize the word as <font color=\"blue\"><b>|walke|</b></font> + remainder <font color=\"blue\"><b>\"d\"</b></font>\n",
    "- We take the next top token \"walk\" and find other untokenized words in the sentence\n",
    "- We repeat the searching and tokenizing steps\n",
    "- The remainder of the word \"walked\", which is <font color=\"blue\"><b>\"d\"</b></font> is matched later with token \"d\", so it is also included in the list of tokenized words\n",
    "- When the token is \"<font color=\"red\"><b>e</b></font>\" and the first untokenized word comes as \"<font color=\"red\"><b>she</b></font>\", the <b>s & h</b> is considered as <b>unknown</b> tokens and are ignored afterwards, so we remain with  |<font color=\"blue\"><b>e</b></font>|\n",
    "- After all the steps, we get the tokenized sentence as <font color=\"blue\"><b>|e| |walke| |d|</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6eb3dcf2-4003-414c-a3c4-1e155098be80",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e79d2302-3983-4e72-a6b0-262d63fd57ad",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "training_data = [\"walker walked a long walk\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d5379f6-be97-4dcf-98a3-060d88d4d3c6",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "bpe_tokenizer = Tokenizer(BPE())\n",
    "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "bpe_trainer = BpeTrainer(vocab_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea4289b9-1761-4dce-8faf-b4be05dab0b2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "bpe_tokenizer.train_from_iterator(training_data, bpe_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57787cee-ffb1-4a0d-9d53-81903d6acaf3",
   "metadata": {},
   "source": [
    "By running <font color=\"blue\">bpe_tokenizer.get_vocab()</font> after tokenizer training, we can see the learned tokens from the training data. As the training process is **iterative**, we can get the order of the tokens learned one after another by looking at their corresponding **ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fd44f86-2aab-46f1-99ec-ea73eac92c7d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 4,\n",
       " 'al': 10,\n",
       " 'l': 5,\n",
       " 'o': 7,\n",
       " 'wal': 11,\n",
       " 'walk': 12,\n",
       " 'e': 2,\n",
       " 'walke': 13,\n",
       " 'r': 8,\n",
       " 'w': 9,\n",
       " 'd': 1,\n",
       " 'g': 3,\n",
       " 'a': 0,\n",
       " 'n': 6}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check what are the learned tokens or vocabulary after the tokenizer training\n",
    "bpe_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a3b18-272e-4ac8-88cd-09407d8d31d2",
   "metadata": {},
   "source": [
    "#### Encode (<font color=\"red\">= Tokenize</font>) new texts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f022ee-a6f0-444f-a7f9-d8448e01db91",
   "metadata": {},
   "source": [
    "This encoder only tries to exact match the tokens out of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74912810-c4ef-4373-86d6-77a095ef01ec",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walke', 'r', 'walke', 'd', 'a', 'l', 'o', 'n', 'g', 'walk']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "671646d9-3403-4ef7-ae20-72af7fbdad96",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 5, 4]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"wlk\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22aced61-6e22-4ebe-bd35-60509e744aa2",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'l', 'k']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e02ebb5-e9e2-4613-948a-655775249ea6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'walke', 'd']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57b19e32-0537-4b5c-80f3-539d083a6376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'al', 'k']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"she talks\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a65b0-5bf1-4b6a-92cd-651e678a8087",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#edfbff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> The <b>Tokenizers</b> library introduces us with different types of tokenizers - <b><font color=\"green\"><b>BPE, WordPiece and Unigram</font></b> <br>\n",
    "Tokenizer library used : <br>\n",
    "https://github.com/huggingface/tokenizers <br>\n",
    "The tokenizers work in the following way <br> </p>\n",
    "<ul style=\"background-color:#edfbff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "    <li style=\"margin-left:10px\">Instantiate the required tokenizer, for example - BPE()</li>\n",
    "    <li style=\"margin-left:10px\">The tokenizer does not have any vocabulary to start with</li>\n",
    "    <li style=\"margin-left:10px\">The Whitespace() pretokenizer splits the text into words based on whitespace seperators</li>\n",
    "    <li style=\"margin-left:10px\">Use a trainer i.e - BpeTrainer</li>\n",
    "    <li style=\"margin-left:10px\">The trainer learns the vocabulary, trains the tokenizer model</li>\n",
    "    <li style=\"margin-left:10px\">Pass the trainer and training_data to the tokenizer and train using a python iterator (train_from_iterator method)</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1fc751-1074-4c56-a68f-ae6e6491c227",
   "metadata": {},
   "source": [
    "## WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db1f12-dd5a-4971-84c4-0bb96088a00e",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#edfbff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "The only difference between <b>WordPiece</b> and <b>BPE</b> is that wordpiece differentiates between initial and middle letters / subword inside a word by adding <b>## prefix</b> to the middle tokens. The general idea of this algorithm is to learn words, prefixes and suffixes separately. The basic steps are the same as BPE, but only t\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45201707-ade9-4996-953d-16a803c5be92",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"img/l2_wordpiece_1.jpg\" width=500px>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be26a0c5-05bc-4c16-bace-904c0247161a",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#fad7ac; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "    <li style=\"margin-left:10px\">Break down the corpus into <b>distinct</b> individual letter tokens</li>\n",
    "    <li style=\"margin-left:10px\">Traverse from left to right of the corpus</li>\n",
    "    <li style=\"margin-left:10px\">At each iteration, analyze a <b><font color=\"green\">pair</font> of single tokens <b>(\"h\", \"##u\" from the word \"hugging\")</b> </li>\n",
    "    <li style=\"margin-left:10px\">Record the score of this pair in a table</li>\n",
    "    <li style=\"margin-left:10px\">Record all such score of sequential pairs in the traversed corpus</li>\n",
    "    <li style=\"margin-left:10px\">Select the highest scoring potential pair, suppose : <b>(\"l\", \"##e\" from the words \"learner\" or \"learn\")</b></li>\n",
    "    <li style=\"margin-left:10px\">Join the pair <b>(\"l\", \"##e\" -> \"le\")</b></li>\n",
    "    <li style=\"margin-left:10px\">Replace all cooccurrences of \"l\" and \"##e\" with \"le\" in the vocabulary</b></li>\n",
    "    <li style=\"margin-left:10px\">Repeat the steps of scoring and updating until reaching the vocabulary size</b></li>\n",
    "<ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c081bd-e69d-4a67-8631-0738a563d2d5",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"img/l2_wordpiece_2.jpg\" width=1000px>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a96f9-b671-4d56-8d0f-b34f6a43b230",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#faecac; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">Let’s say we have the word \"walker\" and the subwords \"wa\", \"lk\", \"er\" and we are at a point where we need to decide what to merge.\n",
    "Suppose [\"wa\"] appears frequently, but [\"wa\"] and [\"lk\"] together aren't as frequent.\n",
    "On the other hand, [\"lk\"] and [\"er\"] occur together often, e.g., in words like \"walker\", \"talker\", \"stalker\".\n",
    "Based on the scoring mechanism, the tokenizer will prioritize merging [\"lk\"] and [\"er\"] into [\"lker\"] (or eventually [\"walk\", \"##er\"]), because this pair maximizes the likelihood of the text, compared to merging [\"wa\"] and [\"lk\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae5228c-c2c9-4aaf-adbe-cddaa2e8e3a7",
   "metadata": {},
   "source": [
    "<p style=\"background-color:orange; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> The real_wordpiece library creates conflict with the existing tokenizers version as it is an older implementation and is <b>almost obsolete</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bbdde63-90d6-493c-8316-96869c366a99",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "from real_wordpiece.trainer import RealWordPieceTrainer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "real_wordpiece_tokenizer = Tokenizer(WordPiece())\n",
    "real_wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# The training algorithm is different than the Huggingface's WordPiece\n",
    "real_wordpiece_trainer = RealWordPieceTrainer(vocab_size=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "daef2453-f14d-4d77-9f33-b9285bd71920",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##n': 10,\n",
       " 'd': 15,\n",
       " 'a': 5,\n",
       " '##ed': 23,\n",
       " '##a': 1,\n",
       " 'o': 16,\n",
       " 'l': 6,\n",
       " '##r': 7,\n",
       " '##k': 3,\n",
       " '##g': 11,\n",
       " 'n': 17,\n",
       " '##d': 8,\n",
       " 'wa': 24,\n",
       " 'lo': 19,\n",
       " '##o': 9,\n",
       " 'r': 14,\n",
       " 'walk': 26,\n",
       " '##er': 22,\n",
       " '##lk': 25,\n",
       " '##ng': 20,\n",
       " 'w': 0,\n",
       " '##e': 4,\n",
       " '##l': 2,\n",
       " 'k': 12,\n",
       " 'g': 18,\n",
       " 'long': 21,\n",
       " 'e': 13}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_wordpiece_trainer.train_tokenizer(training_data, real_wordpiece_tokenizer)\n",
    "real_wordpiece_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29cc85bd-53c6-41b6-8fba-062ba9f82661",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', '##er', 'walk', '##ed', 'a', 'long', 'walk']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_wordpiece_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "455e1f41-e3a1-4366-bfac-44e6ac191aba",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', '##lk']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_wordpiece_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b04960-d9ba-4037-aeab-fe02f1af6d74",
   "metadata": {},
   "source": [
    "**Unknown Characters:**\n",
    "The following line will produce an error because it contains unknown characters. Please uncomment the line and run it to see the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bc73f36-ce2c-45d6-b2e6-834e7b7fe24d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "WordPiece error: Missing [UNK] token from the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mreal_wordpiece_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshe walked\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtokens\n",
      "\u001b[1;31mException\u001b[0m: WordPiece error: Missing [UNK] token from the vocabulary"
     ]
    }
   ],
   "source": [
    "real_wordpiece_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53930c4-e170-463d-b2e4-8def79ce164e",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#faecac; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">The prime difference between <b>RealWordPiece</b> and <b>Huggingface's WordPiece</b> is that the real one uses the <b>scoring function</b> while pairing to effectively detect prefix and suffixes. This enables the tokenization of new words into proper <b>base word and suffixes</b>. But the Huggingface one pairs the base tokens based on frequency of occurrences. <br>\n",
    "\"walker\" -> Huggingface -> \"walker\" (Largest frequently occurring token dominates)<br>\n",
    "\"walker\" -> RealWordPiece -> <b><font color=\"green\">\"walk\"</font></b> \"##er\" (Base word and suffix dominate) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d662e3-29fc-456c-9b75-1a35fdeaf330",
   "metadata": {},
   "source": [
    "## HuggingFace WordPiece and special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d0390-e863-46d5-aeeb-5a9ee2dc2a05",
   "metadata": {},
   "source": [
    "We can include an **UNK** token to handle unknown tokens in our vocab. After training, whenever any given text has any new text in it which is not part of the **trained vocabulary**, it matches with the **UNK** in the vocab then and tokenizes accordingly  \n",
    "In the real wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6c9f6ec-7612-4fc0-9de5-fe9af772e8b4",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "unk_token = \"[UNK]\"\n",
    "\n",
    "wordpiece_model = WordPiece(unk_token=unk_token)\n",
    "wordpiece_tokenizer = Tokenizer(wordpiece_model)\n",
    "wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "wordpiece_trainer = WordPieceTrainer(vocab_size=28, special_tokens=[unk_token]) # 27 + 1 for the new UNK token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e99b0fe-8723-4193-aec4-7af50f4d296d",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##e': 17,\n",
       " 'n': 7,\n",
       " '##g': 16,\n",
       " 'walk': 22,\n",
       " 'l': 6,\n",
       " '##a': 11,\n",
       " 'd': 2,\n",
       " '##d': 19,\n",
       " 'o': 8,\n",
       " '##l': 12,\n",
       " '##ng': 25,\n",
       " 'g': 4,\n",
       " 'r': 9,\n",
       " '##k': 13,\n",
       " '##n': 15,\n",
       " '##lk': 21,\n",
       " 'w': 10,\n",
       " '##r': 18,\n",
       " 'lo': 24,\n",
       " 'walked': 27,\n",
       " 'a': 1,\n",
       " 'e': 3,\n",
       " 'walke': 23,\n",
       " '##o': 14,\n",
       " 'k': 5,\n",
       " 'walker': 26,\n",
       " '[UNK]': 0,\n",
       " 'wa': 20}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.train_from_iterator(\n",
    "    training_data, \n",
    "    wordpiece_trainer\n",
    ")\n",
    "wordpiece_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "594722c8-24b0-496b-9859-943071bc7b78",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walker', 'walked', 'a', 'lo', '##ng', 'walk']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0abca87e-db68-498e-b137-d00a77f034b3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', '##lk']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ccd22755-1115-4d9a-8ddb-3f3b2128941c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'walked']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1863790-b1f7-40a3-9fcd-cc4263530f7b",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c095c-f351-490a-b21c-467fadf8b164",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#edfbff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "    <li style=\"margin-left:10px\">Break down the corpus into all possible list of <b>distinct</b> tokens</li>\n",
    "    <li style=\"margin-left:10px\">Starts with a huge vocabulary (the entire list of possible tokens)</li>\n",
    "    <li style=\"margin-left:10px\">Each token has a likelihood, which is calculated using the <b>frequency</b> of occurrences of that token in the corpus</li>\n",
    "    <li style=\"margin-left:10px\">The corpus likelihood is the product of the probabilities of all the subword tokens in the corpus</li>\n",
    "    <li style=\"margin-left:10px\">This likelihood is maximized by keeping the subwords that contribute most to this probability while pruning low-probability subwords</li>\n",
    "    <li style=\"margin-left:10px\">At each iteration, the token with the <b><font color=\"green\">least reduction in total likelihood of the corpus</font> is removed from the vocab</li>\n",
    "    <li style=\"margin-left:10px\">We keep on reducing and recalculate the corpus likelihood until we reach <b><= vocab size</b></li>\n",
    "<ul> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e5cdd87-35d8-4604-b569-5550696db6d0",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"img/l2_unigram_1.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"img/l2_unigram_2.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "963521e9-bab7-4021-b21a-4775f38c994b",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 9,\n",
       " '[UNK]': 0,\n",
       " 'r': 11,\n",
       " 'd': 8,\n",
       " 'w': 3,\n",
       " 'l': 5,\n",
       " 'walke': 1,\n",
       " 'g': 10,\n",
       " 'n': 12,\n",
       " 'e': 7,\n",
       " 'k': 2,\n",
       " 'a': 6,\n",
       " 'walk': 4}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.models import Unigram\n",
    "\n",
    "unigram_tokenizer = Tokenizer(Unigram())\n",
    "unigram_tokenizer.pre_tokenizer = Whitespace()\n",
    "unigram_trainer = UnigramTrainer(\n",
    "    vocab_size=14, \n",
    "    special_tokens=[unk_token],\n",
    "    unk_token=unk_token,\n",
    ")\n",
    "\n",
    "unigram_tokenizer.train_from_iterator(training_data, unigram_trainer)\n",
    "unigram_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7500af15-5de5-498b-b098-6d1554783257",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walke', 'r', 'walke', 'd', 'a', 'l', 'o', 'n', 'g', 'walk']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93ffe74e-968f-4b37-ab73-5686c18a8af7",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'l', 'k']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee843700-b778-4af3-901c-53d420b12f51",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sh', 'e', 'walke', 'd']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "720a62bb-7713-4e88-acb1-621e1da38203",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 1, 8]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.encode(\"she walked\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9a3e6cc-4787-4922-a0f9-e98ac5eac5b9",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 0]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.encode(\"they\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45b5ccaf-c44b-49ea-a3d5-092beea4ffda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['th', 'e', 'y']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.encode(\"they\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b96fa-4a1e-4a60-b5eb-8f4f97a10387",
   "metadata": {},
   "source": [
    "We can see in the above example that the **\"they\"** has only **\"e\"** as common subword token from the existing vocab, and the parts **\"th\"** and **\"y\"** falls into **[UNK]** token. So the ids of tokens **\"th\" & \"y\"** show **0**, referring to **[UNK]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7db97e61-b26b-4628-bebd-94290afe3d45",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 9,\n",
       " '[UNK]': 0,\n",
       " 'r': 11,\n",
       " 'd': 8,\n",
       " 'w': 3,\n",
       " 'l': 5,\n",
       " 'walke': 1,\n",
       " 'g': 10,\n",
       " 'n': 12,\n",
       " 'e': 7,\n",
       " 'k': 2,\n",
       " 'a': 6,\n",
       " 'walk': 4}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1246017d-6be3-4d78-96ba-dfcd4f979a30",
   "metadata": {
    "height": 30
   },
   "source": [
    "<p style=\"background-color:#faecac; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">The key feature of unigram language modelling tokenizer is that it does not populate any unnecessary <b>glitch tokens</b> like \"##alk\" or \"##al\" or \"##lk\" which do not make any sense for our training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d1d52-baea-4adc-bec1-2886dcb50e18",
   "metadata": {},
   "source": [
    "## SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71394d8f-96c1-4d67-9a4e-d251ae2eaec0",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#faecac; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> This is the same as other algorithms but also consider whitespaces while building the vocabulary. Sometimes we need to consider words like <b>\"Real Madrid\"</b> or <b>San Fransisco</b> as a single token. Sentencepiece can deal with these cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98690e-e6c9-4d8a-994e-d1b15436dc98",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<div>\n",
    "    <img src=\"img/l2_sentencepiece_1.png\" width=400px>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71098902-a3f1-457a-9f0b-a6de29814844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
